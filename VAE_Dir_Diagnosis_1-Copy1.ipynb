{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw 1 sample: <br /> \n",
    "$~~~~~$($\\alpha_1,\\alpha_2,\\alpha_3$)=(1,1,0.5) <br /> \n",
    "$~~~~~$generate random samples ($p_1,p_2,p_3$)$\\sim$Dir($\\alpha_1,\\alpha_2,\\alpha_3$)<br /> \n",
    "Repeat $10^4$ times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualization of 200 samples__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"data2.png\" alt=\"my alt text\" width=300/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code__ in __Appendix A__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"diagnosis1.png\" alt=\"my alt text\" width=500/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_1,...,z_{dim3}\\sim \\mathcal{N}(0,\\mathbb{1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function=$MSE(p,\\hat{p})+KL(Z||\\mathcal{N}(0,1))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code__ in __Appendix B__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generated are too dense (dim1=40, dim2=30, dim3=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"alg1res2.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Possible Reason of Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. overfitting\n",
    "2. inappropriate choice of loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simpler Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Decrease Dimensions of Hidden Layers and Latent Space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim1=4, dim2=2, dim3=1, epoch=20, batch size=50, training samples $10^4$.\n",
    "\n",
    "Sample 200 datapoints after training:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"diag1.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Use Only 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim1=2, dim3=1, epoch=20, batch size=50, training samples $10^4$\n",
    "\n",
    "Sample 200 datapoints after training:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"diag2.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Plot Learning Curves and Set Early Stopping Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split $10^4$ samples to 7000 for training an 3000 for testing, learning curves produced after several attempts:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"learning_curves1.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure>\n",
    "<img src=\"learning_curves2.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</figure> \n",
    "  <img src=\"learning_curves3.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set early stopping epoch to 4, the result will be like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"learning_curve_2.1.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change to $10^5$ samples and split them to 70000 for training an 30000 for testing, learning curves produced after several attempts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"learning_curves_1.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "  <img src=\"learning_curves_3.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code__ in __Appendix C__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Change Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Distance between Distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we have the known true distribution P (Dir(1,1,0.5)), samples $\\{\\pi^{(i)}\\}_{i=1}^{50}$ from P (inputs for one batch), and samples from an unknown distirbution Q and $\\{\\hat{\\pi}^{(i)}\\}_{i=1}^{50}$ (outputs for one batch). $\\pi^{(i)}$ and $\\hat{\\pi}^{(i)}$ are 3 dimensional and 50 is batch size. Want to compute the distance between known true distribution P and unknown distribution Q, samples can be used to estimate the distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.1 Put Output Data to log-Likelihood parameterized from true distribution P and Compute Negative Log-likelihood as Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Funciton = $\\sum_{i=1}^{50}(\\alpha_1-1)log \\hat{\\pi}^{(i)}_1+(\\alpha_2-1)log \\hat{\\pi}^{(i)}_2+(\\alpha_3-1)log \\hat{\\pi}^{(i)}_3$\n",
    "\n",
    "Terms without outputs were ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of outputs after training (Visualization isn't included since it cannot be visualized in triangle plot):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"result2.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "  <figcaption>{ Result with training data set containing 10000 samples from Dir(1,1,0.5),  batch size = 50, hidden layer dim=2 and latent dim =2. }</figcaption>\n",
    "  <img src=\"result1.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "  <figcaption>{ Result with training data set containing 10000 samples from Dir(0.8,0.4,1.2), batch size = 50 , hidden layer dim=2 and latent dim =1.}</figcaption>\n",
    "    \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"learning_curves_3.1.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "  <figcaption>{ Learning curves from training data set containing 7000 samples and testing data set containing 3000 samples from Dir(1,1,0.5), batch size = 50, hidden layer dim=2 and latent dim =1. }</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set early stopping epoch to 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"result10.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "  <figcaption>{ Result with training data set containing 10000 samples from Dir(1,1,0.5),  batch size = 50, hidden layer dim=2 and latent dim =1. }</figcaption>\n",
    " </figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code__ in __Appendix D__ and __Appendix E__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.2 KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we don't know the closed form of Q, we can estimate KL Divergence using K nearest neighbour:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{KL}(P||Q)=\\frac{d}{n}\\sum_{i=1}^nlog\\frac{r_k(\\pi^{(i)})}{s_k(\\pi^{(i)}_i)}+log\\frac{m}{n-1}\n",
    "\\end{equation}\n",
    "where: d is the dimension of samples, which is 3; n and m are batch sizes, which are equal to 50; k means k nearest neighbour; $r_k(\\pi_i)$ and $s_k(\\pi_i)$ are Euclidean distances between $\\pi^{(i)}$ and its k nearest neighbour in $\\{\\hat{\\pi}^{(i)}\\}_{i=1}^{50}$ and $\\{\\pi^{(i)}\\}_{i=1}^{50}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__reference:__\n",
    "\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=502E8F7E57D58F02698CB68AEA325DA6?doi=10.1.1.422.5121&rep=rep1&type=pdf\n",
    "\n",
    "https://www.princeton.edu/~kulkarni/Papers/Journals/j068_2009_WangKulVer_TransIT.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"result6.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "<figcaption>{ Result from training data set containing 10000 samples from Dir(1,1,0.5), batch size = 50, hidden layer dim =2 and latent dim=1. }</figcaption>\n",
    "<img src=\"learning_curve_3.1.2.2.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "<figcaption>{ Learning curves from training data set containing 7000 samples and testing data set containing 3000 samples from Dir(1,1,0.5), batch size = 50, hidden layer dim =2 and latent dim=1. }</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried followiing methods, but they didn't help:\n",
    "1. change batch size to 100,200,1000\n",
    "2. scale reconstruction loss by diviing some constant\n",
    "3. increase sample size\n",
    "4. set early stopping epoch as 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code__ in __Appendix F__ and __Appendix G__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.3 Rényi-$\\alpha$ Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to __3.3.1.2__, Renyi-$\\alpha$ Divergence can be estimated by K nearest neighbour:\n",
    "\\begin{equation}\n",
    "\\hat{R}_\\alpha(P||Q)=\\frac{1}{\\alpha-1}log \\hat{D}_\\alpha(P||Q)=\\frac{1}{\\alpha-1}log\\frac{1}{n}\\sum_{n=1}^n(\\frac{(n-1)r_k(\\pi^{(i)})}{ms_k(\\pi^{(i)})})^{1-\\alpha}B_{k,\\alpha}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where: the notations are the same as before, $B_{k,\\alpha}=\\frac{\\Gamma(k)^2}{\\Gamma(k-\\alpha+1)\\Gamma{k+\\alpha-1)}}$ and $\\alpha$ is a positive value follows Uniform(0,2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reference__:\n",
    "\n",
    "1. http://www.cs.cmu.edu/~schneide/poczos11a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"result7.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "<figcaption>{ Result with 50000 samples, batch size 1000, and epoch 20, take 1st nearest neighbour, $\\alpha$ as 0.5, hidden layer dim = 2 and latent dim = 1. }</figcaption>\n",
    "<img src=\"learning_curve_3.1.3.1.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "<figcaption>{ Learning curve with 50000 samples and batch size 1000, take 1st nearest neighbour, $\\alpha$ as 0.5, hidden layer dim = 2 and latent dim = 1. }</figcaption>\n",
    "<img src=\"result8.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "<figcaption>{ Result with 50000 samples, batch size 1000, and epoch 6, take 1st nearest neighbour, $\\alpha$ as 0.5 , hidden layer dim = 2 and latent dim = 1.}</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code__ in __Appendix H__ and __Appendix I__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried followiing methods, but they didn't help:\n",
    "1. change batch size\n",
    "2. scale reconstruction loss by diviing some constant\n",
    "3. increase sample size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1.4  Maximum Mean Discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two versions of definition of MMD:<br />\n",
    "Ver1:\n",
    "\\begin{equation}\n",
    "D(P||Q)=sup_{f\\in \\mathcal F}E_P(f(x))-E_Q(f(x))\n",
    "\\end{equation}\n",
    "Ver2:\n",
    "\\begin{equation}\n",
    "D(P||Q)=E_{P(x),P(x^\\prime)}(K(x,x^\\prime))+E_{Q(x),Q(x^\\prime)}(K(x,x^\\prime))-2E_{P(x),Q(x^\\prime)}(K(x,x^\\prime))\n",
    "\\end{equation}\n",
    "K is an universal kernel of one's own choice, it can be gaussian, matern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reference__:\n",
    "\n",
    "1. https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "2. http://alex.smola.org/teaching/iconip2006/iconip_3.pdf\n",
    "3. http://jmlr.csail.mit.edu/papers/v13/gretton12a.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"result4.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "  <figcaption>{ Result with 30000 samples, batch size 1000, and epoch 6, take 1st nearest neighbour and $\\alpha$ as 0.5 }</figcaption>\n",
    "   <img src=\"learning_curve_3.1.4.1.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "  <figcaption>{ Learning curve with training data set containing 21000 samples and testing data set containing 9000 samples, batch size 1000, and epoch 20, take 1st nearest neighbour and $\\alpha$ as 0.5}</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set early stopping epoch to 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"result5.png\" alt=\"my alt text\" width=\"300\"/>\n",
    "    <figcaption>{ Learning curve with training data set containing 21000 samples and testing data set containing 9000 samples, batch size 1000, and epoch 2, take 1st nearest neighbour and $\\alpha$ as 0.5}</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried other method, but didn't help:\n",
    "1. increase batch size to 100,200,1000\n",
    "2. change variance in gaussian kernel from 1 to 0.5, 2\n",
    "3. increase sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code__ in __Appendix J__ and __Appendix K__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other\n",
    "1. Hellinger distance: needs to know Q, but Q is unknown\n",
    " \n",
    "2. Total variation distance: needs to know Q, but Q is unknown\n",
    "    \n",
    "3. Jensen–Shannon divergence: needs to know Q, but Q is unknown\n",
    "\n",
    "4. Lévy–Prokhorov metric: needs to know Q, but Q is unknown\n",
    "    \n",
    "5. Bhattacharyya distance: needs to know probability density function of Q\n",
    "\n",
    "6. wasserstein distance: needs to know probability density function of Q\n",
    "\n",
    "7. Kolmogorov–Smirnov: it is difficult to define empirical CDF and theoretical CDF of dirichlet distribution\n",
    "\n",
    "8. Hotelling's T-squared distribution: used in hypothesis test, where null hypothesis is that two samples have same mean. Not useful in our setting\n",
    "\n",
    "9. Kuiper's test: it is difficult to define CDF of dirichlet distribution\n",
    "\n",
    "10. Pearson's chi-squared test: used in hypothesis, where null hypothesis is that expected frequencies is the same as observed frequencies. Not useful in our setting\n",
    "\n",
    "11. student t test: used in hypothesis, where null hypothesis is that expected mean is the same as population mean. Not useful for our setting\n",
    "\n",
    "12. Tukey–Duckworth test: test whether one of two samples was significantly greater than the other, not useful for our setting\n",
    "\n",
    "13. Welch's t-test: used to test the null hypothesis that two populations have equal means, might lead to dense result\n",
    "\n",
    "14. BIC/AIC: not useful in this setting\n",
    "\n",
    "15. Anderson-Darling Test: needs CDF of dirichlet distribution\n",
    "    \n",
    "16. Shapiro–Wilk test: test of normality, not useful\n",
    "\n",
    "17. Hosmer–Lemeshow test: used in logistic regression, not useful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "from scipy.stats import dirichlet\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class Dirdata(Dataset):\n",
    "    def __init__(self, samples=10000,seed=np.random.randint(20),indicate=0,num_param=3):\n",
    "        self.samples = samples\n",
    "        self.seed = seed\n",
    "        self.indicate=indicate\n",
    "        self.num_param=num_param\n",
    "        np.random.seed(self.seed)\n",
    "        self.evalPoints, self.data = self.__simulatedata__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "    \n",
    "    def __getitem__(self, idx=0):\n",
    "        return(self.evalPoints[idx], self.data[idx])\n",
    "    \n",
    "    def __simulatedata__(self):\n",
    "        # Dir(alpha,alpha,alpha), alpha~Uniform(0.5,2)\n",
    "        if (self.indicate==0):\n",
    "            #generate alpha\n",
    "            alpha=np.random.uniform(0,2,self.samples)\n",
    "            #repeat alpha\n",
    "            alpha=np.array([alpha]*self.num_param).transpose()\n",
    "            #initialize theta \n",
    "            theta = np.zeros((self.samples,self.num_param))\n",
    "            #generate theta \n",
    "            for idx in range(self.samples):\n",
    "                #generate theta from dirichlet distr\n",
    "                theta[idx,:]=np.random.dirichlet(alpha[idx,:],1)\n",
    "            return (alpha ,theta)\n",
    "\n",
    "        \n",
    "        # Dir(1,1,0.5)\n",
    "        if (self.indicate==1):\n",
    "            alpha=np.array([1,1,0.5])\n",
    "            #initialize theta \n",
    "            theta = np.zeros((self.samples,self.num_param))\n",
    "            #generate theta\n",
    "            theta=np.random.dirichlet(alpha,self.samples)\n",
    "            return (np.array([alpha]*self.samples).reshape(self.samples,self.num_param) ,theta)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ds =Dirdata(samples=200, indicate=0,num_param=3)\n",
    "    dataloader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    df = pd.DataFrame(columns=['$\\\\theta_1$', '$\\\\theta_2$', '$\\\\theta_3$'])\n",
    "    for no,dt in enumerate(dataloader):\n",
    "        df=df.append(pd.DataFrame(dt[1].numpy(),columns=['$\\\\theta_1$', '$\\\\theta_2$', '$\\\\theta_3$']))\n",
    "    fig =px.scatter_ternary(df, a='$\\\\theta_1$', b='$\\\\theta_2$', c='$\\\\theta_3$',title=\"Dirichlet Distribution Visualization\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard VAE\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "def calculate_loss(reconstructed1,target, mean, log_sd):\n",
    "    RCL = F.mse_loss(reconstructed1, target, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL + KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 50\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 10000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    train_dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ###### train\n",
    "    t = trange(3)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_ = x[1].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_x, z_mu, z_sd = model(x_)\n",
    "            #print(de_out1)\n",
    "            #change dimensionality for computing loss function\n",
    "            reconstructed_x1=reconstructed_x.reshape(batch_size,1,-1)[:,0]\n",
    "            #loss \n",
    "            loss=calculate_loss(reconstructed_x1,x_,z_mu,z_sd)\n",
    "            #compute gradient\n",
    "            loss.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "                \n",
    "            #add to toal loss\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step() # update the weigh\n",
    "        t.set_description(f'Loss is {total_loss/samples:.3}')\n",
    "    \n",
    "    ###### Sampling 200 draws from learnt model\n",
    "    model.eval() # model in eval mode\n",
    "    z = torch.randn(200, z_dim).to(device) # random draw\n",
    "    with torch.no_grad():\n",
    "        sampled_y = model.decoder(z)\n",
    "    df=pd.DataFrame(sampled_y,columns=['$\\\\theta_1$', '$\\\\theta_2$', '$\\\\theta_3$'])\n",
    "    fig =px.scatter_ternary(df, a='$\\\\theta_1$', b='$\\\\theta_2$', c='$\\\\theta_3$',title=\"Dirichlet Distribution Visualization\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning curve for standard VAE \n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "def calculate_loss(reconstructed1,target, mean, log_sd):\n",
    "    RCL = F.mse_loss(reconstructed1, target, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL + KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 50\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 10000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "    train_samples = 7000\n",
    "    test_samples = 3000\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    #split to training and testing set\n",
    "    train_dl,test_dl = torch.utils.data.random_split(dl.dataset, (train_samples, test_samples))\n",
    "    train_dl = DataLoader(train_dl, batch_size=batch_size)\n",
    "    test_dl = DataLoader(test_dl, batch_size=batch_size)\n",
    "    \n",
    "    ###### train\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    t = trange(20)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_train = x[1].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_train_x, z_train_mu, z_train_sd = model(x_train)\n",
    "            #change dimensionality for computing loss function\n",
    "            reconstructed_train_x1=reconstructed_train_x.reshape(batch_size,1,-1)[:,0]\n",
    "            #loss \n",
    "            loss_train=calculate_loss(reconstructed_train_x1,x_train,z_train_mu,z_train_sd)\n",
    "            #compute gradient\n",
    "            loss_train.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "            #add to toal loss\n",
    "            total_train_loss += loss_train.item()\n",
    "            optimizer.step() # update the weight\n",
    "        train_loss.append(total_train_loss/train_samples)\n",
    "        #validate\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        for i,x in enumerate(test_dl):\n",
    "            #input\n",
    "            x_test = x[1].float().to(device)\n",
    "            #output of VAE\n",
    "            reconstructed_test_x, z_test_mu, z_test_sd = model(x_test)\n",
    "            reconstructed_test_x1=reconstructed_test_x.reshape(batch_size,1,-1)[:,0]\n",
    "            #compute loss\n",
    "            loss_test=calculate_loss(reconstructed_test_x1,x_test,z_test_mu, z_test_sd)\n",
    "            #add loss to total loss\n",
    "            total_test_loss += loss_test.item()\n",
    "        test_loss.append(total_test_loss/test_samples)\n",
    "        t.set_description(f'Train Loss is {total_train_loss/train_samples:.3}')\n",
    "    #plot learning curves\n",
    "    fig1 = plt.figure(figsize=(10,8))\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.plot(np.linspace(1,20,20,endpoint=True),np.array(train_loss).reshape(20,),label=\"Training\")\n",
    "    ax1.plot(np.linspace(1,20,20,endpoint=True),np.array(test_loss).reshape(20,),label=\"Testing\")\n",
    "    plt.xticks(np.arange(min(np.linspace(1,20,20,endpoint=True)), max(np.linspace(1,20,20,endpoint=True))+1, 1.0))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Cruves\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curves_2.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put Output Data to log-Likelihood and Compute Negative Log-likelihood as Loss Function\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "def calculate_loss(likeli, mean, log_sd):\n",
    "    RCL = -torch.sum(likeli)\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL, KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 50\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 10000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    train_dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ###### train\n",
    "    t = trange(20)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_ = x[1].float().to(device)\n",
    "            alpha = x[0].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_x, z_mu, z_sd = model(x_)\n",
    "            #log-likelihood, ignore irrelavent parts, need to be summed later\n",
    "            likeli=torch.mul(torch.log(reconstructed_x+1e-7),alpha-1)\n",
    "            #loss \n",
    "            rcl,kld=calculate_loss(likeli,z_mu,z_sd)\n",
    "            loss=rcl+kld\n",
    "            #compute gradient\n",
    "            loss.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "                \n",
    "            #add to toal loss\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step() # update the weigh\n",
    "        t.set_description(f'Loss is {total_loss/samples:.3}')\n",
    "    \n",
    "    ###### Sampling 5 draws from learnt model\n",
    "    model.eval() # model in eval mode\n",
    "    z = torch.randn(200, z_dim).to(device) # random draw\n",
    "    with torch.no_grad():\n",
    "        sampled_y = model.decoder(z)\n",
    "    print(sampled_y)\n",
    "    df=pd.DataFrame(sampled_y,columns=['$\\\\theta_1$', '$\\\\theta_2$', '$\\\\theta_3$'])\n",
    "    fig =px.scatter_ternary(df, a='$\\\\theta_1$', b='$\\\\theta_2$', c='$\\\\theta_3$',title=\"Dirichlet Distribution Visualization\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put Output Data to log-Likelihood and Compute Negative Log-likelihood as Loss Function\n",
    "#+learning curve\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "def calculate_loss(likeli, mean, log_sd):\n",
    "    RCL = -torch.sum(likeli)\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL, KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 50\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 10000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "    train_samples = 7000\n",
    "    test_samples = 3000\n",
    "\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    #split to training and testing set\n",
    "    train_dl,test_dl = torch.utils.data.random_split(dl.dataset, (train_samples, test_samples))\n",
    "    train_dl = DataLoader(train_dl, batch_size=batch_size)\n",
    "    test_dl = DataLoader(test_dl, batch_size=batch_size)\n",
    "    \n",
    "    ###### train\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    t = trange(30)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_train = x[1].float().to(device)\n",
    "            alpha_train = x[0].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_train_x, z_train_mu, z_train_sd = model(x_train)\n",
    "            likeli_train=torch.mul(torch.log(reconstructed_train_x+1e-7),alpha_train-1)\n",
    "            #change dimensionality for computing loss function\n",
    "            rcl_train,kld_train=calculate_loss(likeli_train,z_train_mu,z_train_sd)\n",
    "            #loss \n",
    "            loss_train=rcl_train+kld_train\n",
    "            #compute gradient\n",
    "            loss_train.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "            #add to toal loss\n",
    "            total_train_loss += loss_train.item()\n",
    "            optimizer.step() # update the weight\n",
    "        train_loss.append(total_train_loss/train_samples)\n",
    "        #validate\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        for i,x in enumerate(test_dl):\n",
    "            #input\n",
    "            x_test = x[1].float().to(device)\n",
    "            alpha_test = x[0].float().to(device)\n",
    "            #output of VAE\n",
    "            reconstructed_test_x, z_test_mu, z_test_sd = model(x_test)\n",
    "            likeli_test=torch.mul(torch.log(reconstructed_test_x+1e-7),alpha_test-1)\n",
    "            #loss \n",
    "            rcl_test,kld_test=calculate_loss(likeli_test,z_test_mu, z_test_sd)\n",
    "            loss_test=rcl_test+kld_test\n",
    "            #add loss to total loss\n",
    "            total_test_loss += loss_test.item()\n",
    "        test_loss.append(total_test_loss/test_samples)\n",
    "        t.set_description(f'Train Loss is {total_train_loss/train_samples:.3}')\n",
    "    #plot learning curves\n",
    "    fig1 = plt.figure(figsize=(10,8))\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.plot(np.linspace(1,30,30,endpoint=True),np.array(train_loss).reshape(30,),label=\"Training\")\n",
    "    ax1.plot(np.linspace(1,30,30,endpoint=True),np.array(test_loss).reshape(30,),label=\"Testing\")\n",
    "    plt.xticks(np.arange(min(np.linspace(1,30,30,endpoint=True)), max(np.linspace(1,30,30,endpoint=True))+1, 1.0))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Cruves\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curve_3.1.1.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate kl\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "#compute the distance between pi and pi_hat\n",
    "def compute_distance(x, y):\n",
    "    x_size = x.shape[0]\n",
    "    y_size = y.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    tiled_x = x.transpose(0,1).repeat(y_size,1).transpose(0,1).reshape(x_size,y_size,dim)\n",
    "    tiled_y = y.reshape(1,y_size,dim).repeat(x_size,1,1)\n",
    "    return torch.sqrt(torch.sum((tiled_x - tiled_y)**2,dim=2))\n",
    "\n",
    "#prepare to find knn of x\n",
    "def KNN(x,y):\n",
    "    sorted,indicies=torch.sort(compute_distance(x,y))\n",
    "    return sorted\n",
    "\n",
    "def calculate_loss(output,target, mean, log_sd,K):\n",
    "    RCL = torch.sum(torch.log(torch.mul(KNN(target,output)[:,K-1],1/KNN(target,target)[:,K])))*output.shape[1]/output.shape[0]+np.log(output.shape[0]/(output.shape[0]-1))\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL, KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 1000\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 10000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "    K = 2\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    train_dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ###### train\n",
    "    t = trange(20)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_ = x[1].float().to(device)\n",
    "            alpha = x[0].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_x, z_mu, z_sd = model(x_)\n",
    "            #log-likelihood, ignore irrelavent parts, need to be summed later\n",
    "            #loss \n",
    "            rcl,kld=calculate_loss(reconstructed_x,x_,z_mu,z_sd,K)\n",
    "            loss=rcl+kld\n",
    "            #compute gradient\n",
    "            loss.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "                \n",
    "            #add to toal loss\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step() # update the weigh\n",
    "        t.set_description(f'Loss is {total_loss/samples:.3}')\n",
    "    \n",
    "    ###### Sampling 5 draws from learnt model\n",
    "    model.eval() # model in eval mode\n",
    "    z = torch.randn(200, z_dim).to(device) # random draw\n",
    "    with torch.no_grad():\n",
    "        sampled_y = model.decoder(z)\n",
    "    print(sampled_y)\n",
    "    df=pd.DataFrame(sampled_y,columns=['$\\\\theta_1$', '$\\\\theta_2$', '$\\\\theta_3$'])\n",
    "    fig =px.scatter_ternary(df, a='$\\\\theta_1$', b='$\\\\theta_2$', c='$\\\\theta_3$',title=\"Dirichlet Distribution Visualization\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate kl + learning curve\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "def compute_distance(x, y):\n",
    "    x_size = x.shape[0]\n",
    "    y_size = y.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    tiled_x = x.transpose(0,1).repeat(y_size,1).transpose(0,1).reshape(x_size,y_size,dim)\n",
    "    tiled_y = y.reshape(1,y_size,dim).repeat(x_size,1,1)\n",
    "    return torch.sqrt(torch.sum((tiled_x - tiled_y)**2,dim=2))\n",
    "\n",
    "def KNN(x,y):\n",
    "    sorted,indicies=torch.sort(compute_distance(x,y))\n",
    "    return sorted\n",
    "\n",
    "def calculate_loss(output,target, mean, log_sd,K):\n",
    "    RCL = torch.sum(torch.log(torch.mul(KNN(target,output)[:,K-1],1/KNN(target,target)[:,K])))*output.shape[1]/output.shape[0]+np.log(output.shape[0]/(output.shape[0]-1))\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL/50, KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 50\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 20000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "    train_samples = 14000\n",
    "    test_samples = 6000\n",
    "    K=1\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    # split to training set and testing set\n",
    "    train_dl,test_dl = torch.utils.data.random_split(dl.dataset, (train_samples, test_samples))\n",
    "    train_dl = DataLoader(train_dl, batch_size=batch_size)\n",
    "    test_dl = DataLoader(test_dl, batch_size=batch_size)\n",
    "    \n",
    "    ###### train\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    t = trange(20)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_train = x[1].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_train_x, z_train_mu, z_train_sd = model(x_train)\n",
    "            #change dimensionality for computing loss function\n",
    "            rcl_train,kld_train=calculate_loss(reconstructed_train_x,x_train,z_train_mu,z_train_sd,K)\n",
    "            #loss \n",
    "            loss_train=rcl_train+kld_train\n",
    "            #compute gradient\n",
    "            loss_train.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "            #add to toal loss\n",
    "            total_train_loss += loss_train.item()\n",
    "            optimizer.step() # update the weight\n",
    "        train_loss.append(total_train_loss/train_samples)\n",
    "        #validate\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        for i,x in enumerate(test_dl):\n",
    "            #input\n",
    "            x_test = x[1].float().to(device)\n",
    "            #output of VAE\n",
    "            reconstructed_test_x, z_test_mu, z_test_sd = model(x_test)\n",
    "            rcl_test,kld_test=calculate_loss(reconstructed_test_x,x_test,z_test_mu,z_test_sd,K)\n",
    "            #loss \n",
    "            loss_test=rcl_test+kld_test\n",
    "            #add loss to total loss\n",
    "            total_test_loss += loss_test.item()\n",
    "        test_loss.append(total_test_loss/test_samples)\n",
    "        t.set_description(f'Train Loss is {total_train_loss/train_samples:.3}')\n",
    "    #plot learning curves\n",
    "    fig1 = plt.figure(figsize=(10,8))\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.plot(np.linspace(1,20,20,endpoint=True),np.array(train_loss).reshape(20,),label=\"Training\")\n",
    "    ax1.plot(np.linspace(1,20,20,endpoint=True),np.array(test_loss).reshape(20,),label=\"Testing\")\n",
    "    plt.xticks(np.arange(min(np.linspace(1,20,20,endpoint=True)), max(np.linspace(1,20,20,endpoint=True))+1, 1.0))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Cruves\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curve_3.1.2.2.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate renyi-alpha\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "#compute distance between x and y\n",
    "def compute_distance(x, y):\n",
    "    x_size = x.shape[0]\n",
    "    y_size = y.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    tiled_x = x.transpose(0,1).repeat(y_size,1).transpose(0,1).reshape(x_size,y_size,dim)\n",
    "    tiled_y = y.reshape(1,y_size,dim).repeat(x_size,1,1)\n",
    "    return torch.sqrt(torch.sum((tiled_x - tiled_y)**2,dim=2))\n",
    "\n",
    "#prepare for K nearest neighbour \n",
    "def KNN(x,y):\n",
    "    sorted,indicies=torch.sort(compute_distance(x,y))\n",
    "    return sorted\n",
    "\n",
    "def calculate_loss(output,target, mean, log_sd,K,a):\n",
    "    RCL = 1/(a-1)*torch.log(1/output.shape[0]*torch.sum((((output.shape[0]-1)/output.shape[0])**(1-a))*torch.mul(KNN(target,target)[:,K],1/KNN(target,output)[:,K-1]).pow(1-a)))\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL, KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 1000\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 50000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "    K = 1\n",
    "    a = 0.5\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    train_dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ###### train\n",
    "    t = trange(4)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_ = x[1].float().to(device)\n",
    "            alpha = x[0].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_x, z_mu, z_sd = model(x_)\n",
    "            #log-likelihood, ignore irrelavent parts, need to be summed later\n",
    "            #loss \n",
    "            rcl,kld=calculate_loss(reconstructed_x,x_,z_mu,z_sd,K,a)\n",
    "            #print(rcl,kld)\n",
    "            loss=rcl+kld\n",
    "            #compute gradient\n",
    "            loss.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "                \n",
    "            #add to toal loss\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step() # update the weigh\n",
    "        t.set_description(f'Loss is {total_loss/samples:.3}')\n",
    "    \n",
    "    ###### Sampling 5 draws from learnt model\n",
    "    model.eval() # model in eval mode\n",
    "    z = torch.randn(200, z_dim).to(device) # random draw\n",
    "    with torch.no_grad():\n",
    "        sampled_y = model.decoder(z)\n",
    "    print(sampled_y)\n",
    "    df=pd.DataFrame(sampled_y,columns=['$\\\\theta_1$', '$\\\\theta_2$', '$\\\\theta_3$'])\n",
    "    fig =px.scatter_ternary(df, a='$\\\\theta_1$', b='$\\\\theta_2$', c='$\\\\theta_3$',title=\"Dirichlet Distribution Visualization\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate renyi-alpha+learning curves\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "#compute distance between x and y\n",
    "def compute_distance(x, y):\n",
    "    x_size = x.shape[0]\n",
    "    y_size = y.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    tiled_x = x.transpose(0,1).repeat(y_size,1).transpose(0,1).reshape(x_size,y_size,dim)\n",
    "    tiled_y = y.reshape(1,y_size,dim).repeat(x_size,1,1)\n",
    "    return torch.sqrt(torch.sum((tiled_x - tiled_y)**2,dim=2))\n",
    "\n",
    "#prepare for K nearest neighbour \n",
    "def KNN(x,y):\n",
    "    sorted,indicies=torch.sort(compute_distance(x,y))\n",
    "    return sorted\n",
    "\n",
    "def calculate_loss(output,target, mean, log_sd,K,a):\n",
    "    RCL = 1/(a-1)*torch.log(1/output.shape[0]*torch.sum((((output.shape[0]-1)/output.shape[0])**(1-a))*torch.mul(KNN(target,target)[:,K],1/KNN(target,output)[:,K-1]).pow(1-a)))\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL, KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 1000\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 50000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "    train_samples = 35000\n",
    "    test_samples = 15000\n",
    "    K=1\n",
    "    a=0.5\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    #split to training set and testing set \n",
    "    train_dl,test_dl = torch.utils.data.random_split(dl.dataset, (train_samples, test_samples))\n",
    "    train_dl = DataLoader(train_dl, batch_size=batch_size, shuffle=True)\n",
    "    test_dl = DataLoader(test_dl, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ###### train\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    t = trange(20)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_train = x[1].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_train_x, z_train_mu, z_train_sd = model(x_train)\n",
    "            #change dimensionality for computing loss function\n",
    "            rcl_train,kld_train=calculate_loss(reconstructed_train_x,x_train,z_train_mu,z_train_sd,K,a)\n",
    "            #loss \n",
    "            loss_train=rcl_train+kld_train\n",
    "            #compute gradient\n",
    "            loss_train.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "            #add to toal loss\n",
    "            total_train_loss += loss_train.item()\n",
    "            optimizer.step() # update the weight\n",
    "        train_loss.append(total_train_loss/train_samples)\n",
    "        #validate\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        for i,x in enumerate(test_dl):\n",
    "            #input\n",
    "            x_test = x[1].float().to(device)\n",
    "            #output of VAE\n",
    "            reconstructed_test_x, z_test_mu, z_test_sd = model(x_test)\n",
    "            rcl_test,kld_test=calculate_loss(reconstructed_test_x,x_test,z_test_mu,z_test_sd,K,a)\n",
    "            #loss \n",
    "            loss_test=rcl_test+kld_test\n",
    "            #add loss to total loss\n",
    "            total_test_loss += loss_test.item()\n",
    "        test_loss.append(total_test_loss/test_samples)\n",
    "        t.set_description(f'Train Loss is {total_train_loss/train_samples:.3}')\n",
    "    #plot learning curves\n",
    "    fig1 = plt.figure(figsize=(10,8))\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.plot(np.linspace(1,20,20,endpoint=True),np.array(train_loss).reshape(20,),label=\"Training\")\n",
    "    ax1.plot(np.linspace(1,20,20,endpoint=True),np.array(test_loss).reshape(20,),label=\"Testing\")\n",
    "    plt.xticks(np.arange(min(np.linspace(1,20,20,endpoint=True)), max(np.linspace(1,20,20,endpoint=True))+1, 1.0))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Cruves\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curve_3.1.3.2.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mmd\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "# compute distance between samples in x and samples in y\n",
    "def compute_kernel(x, y):\n",
    "    x_size = x.shape[0]\n",
    "    y_size = y.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    tiled_x = x.transpose(0,1).repeat(y_size,1).transpose(0,1).reshape(x_size,y_size,dim)\n",
    "    tiled_y = y.reshape(1,y_size,dim).repeat(x_size,1,1)\n",
    "    return  torch.exp(-torch.sum((tiled_x - tiled_y)**2,dim=2)/1)\n",
    "\n",
    "# compute mmd\n",
    "def compute_mmd(x, y, sigma_sqr=1.0):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return torch.mean( x_kernel)+torch.mean(y_kernel)-2*torch.mean(xy_kernel)\n",
    "    \n",
    "def calculate_loss(mmd, mean, log_sd):\n",
    "    RCL = -mmd\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL, KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 1000\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 10000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    train_dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ###### train\n",
    "    t = trange(8)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_ = x[1].float().to(device)\n",
    "            alpha = x[0].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_x, z_mu, z_sd = model(x_)\n",
    "            #conpute mmd\n",
    "            mmd = compute_mmd(x_, reconstructed_x, sigma_sqr=1.0)\n",
    "            #loss \n",
    "            mmd,kld=calculate_loss(mmd,z_mu,z_sd)\n",
    "            loss=mmd+kld\n",
    "            #compute gradient\n",
    "            loss.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "                \n",
    "            #add to toal loss\n",
    "            total_loss += loss.item()\n",
    "            optimizer.step() # update the weigh\n",
    "        t.set_description(f'Loss is {total_loss/samples:.3}')\n",
    "    \n",
    "    ###### Sampling 5 draws from learnt model\n",
    "    model.eval() # model in eval mode\n",
    "    z = torch.randn(200, z_dim).to(device) # random draw\n",
    "    with torch.no_grad():\n",
    "        sampled_y = model.decoder(z)\n",
    "    print(sampled_y)\n",
    "    df=pd.DataFrame(sampled_y,columns=['$\\\\theta_1$', '$\\\\theta_2$', '$\\\\theta_3$'])\n",
    "    fig =px.scatter_ternary(df, a='$\\\\theta_1$', b='$\\\\theta_2$', c='$\\\\theta_3$',title=\"Dirichlet Distribution Visualization\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mmd+learning curve\n",
    "from scipy.stats import dirichlet as diri\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import math\n",
    "from scipy.special import gamma, factorial\n",
    "from tqdm import tqdm, trange\n",
    "from Dirdata2 import Dirdata\n",
    "import torch\n",
    "from scipy.stats import multinomial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import dirichlet\n",
    "import os,sys\n",
    "import pystan\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.mu = nn.Linear(hidden_dim1, z_dim)\n",
    "        self.sd = nn.Linear(hidden_dim1, z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim1]\n",
    "        z_mu = self.mu(hidden1)\n",
    "        # z_mu is of shape [batch_size, z_dim]\n",
    "        z_sd = self.sd(hidden1)\n",
    "        # z_sd is of shape [batch_size, z_dim]\n",
    "        return z_mu, z_sd\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "    '''\n",
    "    def __init__(self,z_dim, hidden_dim1, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden_dim1)\n",
    "        self.out1 = nn.Linear(hidden_dim1, input_dim)\n",
    "        self.out2 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, z_dim]\n",
    "        hidden1 = self.linear1(x)\n",
    "        # hidden1 is of shape [batch_size, hidden_dim2]\n",
    "        out1 = self.out1(hidden1)\n",
    "        #ensure sum of 3 elements to be 1\n",
    "        pred = self.out2(out1)\n",
    "        # pred is of shape [batch_size, input_dim]\n",
    "        return pred\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim1,  latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim1,  latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim1,  input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_sd):\n",
    "        '''During training random sample from the learned ZDIMS-dimensional\n",
    "           normal distribution; during inference its mean.\n",
    "        '''\n",
    "        if self.training:\n",
    "            # sample from the distribution having latent parameters z_mu, z_sd\n",
    "            # reparameterize\n",
    "            std = torch.exp(z_sd / 2)\n",
    "            eps = torch.randn_like(std)\n",
    "            return (eps.mul(std).add_(z_mu))\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_sd = self.encoder(x)\n",
    "        # reparameterize\n",
    "        x_sample = self.reparameterize(z_mu, z_sd)\n",
    "        # decode\n",
    "        generated_x = self.decoder(x_sample)\n",
    "        return generated_x, z_mu,z_sd\n",
    "\n",
    "# compute distance between samples in x and samples in y\n",
    "def compute_kernel(x, y):\n",
    "    x_size = x.shape[0]\n",
    "    y_size = y.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    tiled_x = x.transpose(0,1).repeat(y_size,1).transpose(0,1).reshape(x_size,y_size,dim)\n",
    "    tiled_y = y.reshape(1,y_size,dim).repeat(x_size,1,1)\n",
    "    return  torch.exp(-torch.sum((tiled_x - tiled_y)**2,dim=2)/1)\n",
    "\n",
    "# compute mmd\n",
    "def compute_mmd(x, y, sigma_sqr=1.0):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return torch.mean( x_kernel)+torch.mean(y_kernel)-2*torch.mean(xy_kernel)\n",
    "    \n",
    "def calculate_loss(mmd, mean, log_sd):\n",
    "    RCL = -mmd\n",
    "    KLD = -0.5 * torch.sum(1 + log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL, KLD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ###### intializing data and model parameters\n",
    "    batch_size = 1000\n",
    "    hidden_dim1 = 2\n",
    "    z_dim = 1\n",
    "    samples = 30000\n",
    "    num_param=3\n",
    "    input_dim = 3\n",
    "    train_samples = 21000\n",
    "    test_samples = 9000\n",
    "\n",
    "    model = VAE(input_dim, hidden_dim1, z_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "    model = model.to(device)\n",
    "    \n",
    "    ###### creating data\n",
    "    ds =Dirdata(samples=samples, indicate=1,num_param=num_param)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    #split training set and testing set\n",
    "    train_dl,test_dl = torch.utils.data.random_split(dl.dataset, (train_samples, test_samples))\n",
    "    train_dl = DataLoader(train_dl, batch_size=batch_size)\n",
    "    test_dl = DataLoader(test_dl, batch_size=batch_size)\n",
    "    \n",
    "    ###### train\n",
    "    train_loss=[]\n",
    "    test_loss=[]\n",
    "    t = trange(30)\n",
    "    for e in t:\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for i,x in enumerate(train_dl):\n",
    "            #input for VAE (flattened)\n",
    "            x_train = x[1].float().to(device)\n",
    "            #make gradient to be zero in each loop\n",
    "            optimizer.zero_grad()\n",
    "            #get output\n",
    "            reconstructed_train_x, z_train_mu, z_train_sd = model(x_train)\n",
    "            #change dimensionality for computing loss function\n",
    "            mmd_train = compute_mmd(x_train, reconstructed_train_x, sigma_sqr=1.0)\n",
    "            mmd_train,kld_train=calculate_loss(mmd_train,z_train_mu, z_train_sd)\n",
    "            #loss \n",
    "            loss_train=mmd_train+kld_train\n",
    "            #compute gradient\n",
    "            loss_train.backward() \n",
    "            #if gradient is nan, change to 0\n",
    "            for param in model.parameters():\n",
    "                param.grad[param.grad!=param.grad]=0\n",
    "            #add to toal loss\n",
    "            total_train_loss += loss_train.item()\n",
    "            optimizer.step() # update the weight\n",
    "        train_loss.append(total_train_loss/train_samples)\n",
    "        #validate\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        for i,x in enumerate(test_dl):\n",
    "            #input\n",
    "            x_test = x[1].float().to(device)\n",
    "            #output of VAE\n",
    "            reconstructed_test_x, z_test_mu, z_test_sd = model(x_test)\n",
    "            mmd_test = compute_mmd(x_test, reconstructed_test_x, sigma_sqr=1.0)\n",
    "            mmd_test,kld_test=calculate_loss(mmd_test,z_test_mu, z_test_sd)\n",
    "            #loss \n",
    "            mmd_test,kld_test=calculate_loss(mmd_test,z_test_mu, z_test_sd)\n",
    "            loss_test=mmd_test+kld_test\n",
    "            #add loss to total loss\n",
    "            total_test_loss += loss_test.item()\n",
    "        test_loss.append(total_test_loss/test_samples)\n",
    "        t.set_description(f'Train Loss is {total_train_loss/train_samples:.3}')\n",
    "    #plot learning curves\n",
    "    fig1 = plt.figure(figsize=(10,8))\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    ax1.plot(np.linspace(1,30,30,endpoint=True),np.array(train_loss).reshape(30,),label=\"Training\")\n",
    "    ax1.plot(np.linspace(1,30,30,endpoint=True),np.array(test_loss).reshape(30,),label=\"Testing\")\n",
    "    plt.xticks(np.arange(min(np.linspace(1,30,30,endpoint=True)), max(np.linspace(1,30,30,endpoint=True))+1, 1.0))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Learning Cruves\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curve_3.1.4.1.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
